{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eece26e-5166-4efe-b2c7-b44f01fd6f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, year, month, avg, count, max, min, \n",
    "    sum as spark_sum, stddev, round as spark_round,\n",
    "    desc, asc, sqrt, pow as spark_pow, lit, \n",
    "    udf, array, struct, explode, window, to_timestamp,\n",
    "    date_format, dayofweek, quarter, weekofyear,\n",
    "    coalesce, isnan, isnull, abs as spark_abs\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, FloatType, IntegerType, \n",
    "    DoubleType, StringType, BooleanType\n",
    ")\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import (\n",
    "    BinaryClassificationEvaluator, \n",
    "    MulticlassClassificationEvaluator,\n",
    "    RegressionEvaluator\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "114c63ab-78c3-4051-a9b4-48e37b50c082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "earthquake_schema = StructType([\n",
    "    StructField(\"magnitude\", FloatType(), True),\n",
    "    StructField(\"cdi\", FloatType(), True),\n",
    "    StructField(\"mmi\", FloatType(), True),\n",
    "    StructField(\"sig\", IntegerType(), True),\n",
    "    StructField(\"nst\", IntegerType(), True),\n",
    "    StructField(\"dmin\", FloatType(), True),\n",
    "    StructField(\"gap\", FloatType(), True),\n",
    "    StructField(\"depth\", FloatType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Month\", IntegerType(), True),\n",
    "    StructField(\"tsunami\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "651dc92d-cde9-4d71-b419-c7836c39f494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(earthquake_schema) \\\n",
    "    .csv(\"/Volumes/workspace/default/data/earthquake_data_tsunami.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "364587fd-25a7-4bc6-af87-9eb946669c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- magnitude: float (nullable = true)\n |-- cdi: float (nullable = true)\n |-- mmi: float (nullable = true)\n |-- sig: integer (nullable = true)\n |-- nst: integer (nullable = true)\n |-- dmin: float (nullable = true)\n |-- gap: float (nullable = true)\n |-- depth: float (nullable = true)\n |-- latitude: float (nullable = true)\n |-- longitude: float (nullable = true)\n |-- Year: integer (nullable = true)\n |-- Month: integer (nullable = true)\n |-- tsunami: integer (nullable = true)\n\n+---------+---+---+---+---+-----+----+-------+--------+---------+----+-----+-------+\n|magnitude|cdi|mmi|sig|nst|dmin |gap |depth  |latitude|longitude|Year|Month|tsunami|\n+---------+---+---+---+---+-----+----+-------+--------+---------+----+-----+-------+\n|7.0      |8.0|7.0|768|117|0.509|17.0|14.0   |-9.7963 |159.596  |2022|11   |1      |\n|6.9      |4.0|4.0|735|99 |2.229|34.0|25.0   |-4.9559 |100.738  |2022|11   |0      |\n|7.0      |3.0|3.0|755|147|3.125|18.0|579.0  |-20.0508|-178.346 |2022|11   |1      |\n|7.3      |5.0|5.0|833|149|1.865|21.0|37.0   |-19.2918|-172.129 |2022|11   |1      |\n|6.6      |0.0|2.0|670|131|4.998|27.0|624.464|-25.5948|178.278  |2022|11   |1      |\n+---------+---+---+---+---+-----+----+-------+--------+---------+----+-----+-------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_raw.printSchema()\n",
    "df_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46563cf3-5462-4752-a5f6-1f9f4e925441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transformation 1: calculated columns\n",
    "\n",
    "\n",
    "df_transformed = df_raw \\\n",
    "    .withColumn(\"magnitude_category\", \n",
    "                when(col(\"magnitude\") < 5.0, \"Minor\")\n",
    "                .when((col(\"magnitude\") >= 5.0) & (col(\"magnitude\") < 6.0), \"Light\")\n",
    "                .when((col(\"magnitude\") >= 6.0) & (col(\"magnitude\") < 7.0), \"Moderate\")\n",
    "                .when((col(\"magnitude\") >= 7.0) & (col(\"magnitude\") < 8.0), \"Major\")\n",
    "                .otherwise(\"Great\")) \\\n",
    "    .withColumn(\"depth_category\",\n",
    "                when(col(\"depth\") < 70, \"Shallow\")\n",
    "                .when((col(\"depth\") >= 70) & (col(\"depth\") < 300), \"Intermediate\")\n",
    "                .otherwise(\"Deep\")) \\\n",
    "    .withColumn(\"distance_from_equator\", \n",
    "                sqrt(spark_pow(col(\"latitude\"), 2))) \\\n",
    "    .withColumn(\"tsunami_risk_score\",\n",
    "                when(col(\"tsunami\") == 1, col(\"magnitude\") * 10 + col(\"depth\") / 10)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"seismic_intensity\",\n",
    "                col(\"magnitude\") * col(\"sig\") / 100) \\\n",
    "    .withColumn(\"quarter\", quarter(lit(f\"{df_raw.select('Year').first()[0]}-01-01\"))) \\\n",
    "    .withColumn(\"hemisphere\",\n",
    "                when(col(\"latitude\") >= 0, \"Northern\").otherwise(\"Southern\")) \\\n",
    "    .withColumn(\"ocean_region\",\n",
    "                when((col(\"longitude\") >= -180) & (col(\"longitude\") < -30), \"Atlantic\")\n",
    "                .when((col(\"longitude\") >= -30) & (col(\"longitude\") < 75), \"Indian\")\n",
    "                .when((col(\"longitude\") >= 75) & (col(\"longitude\") <= 180), \"Pacific\")\n",
    "                .otherwise(\"Unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e07456d1-234c-4e6d-b35a-a5b49aa69441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major earthquakes (mag >= 6.5): 782\n"
     ]
    }
   ],
   "source": [
    "# Filter 1: major earthquakes (magnitude >= 6.5)\n",
    "df_major = df_transformed.filter(col(\"magnitude\") >= 6.5)\n",
    "print(f\"Major earthquakes (mag >= 6.5): {df_major.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a5c959b-1764-4c98-bc7c-7cacf87cc132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tsunami-generating earthquakes: 304\n"
     ]
    }
   ],
   "source": [
    "# Filter 2: tsunami-generating earthquakes\n",
    "df_tsunami = df_transformed.filter(col(\"tsunami\") == 1)\n",
    "print(f\"Tsunami-generating earthquakes: {df_tsunami.count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb04c4dc-ffa0-4516-a1c1-c82fcba2901a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent earthquakes (2021-2022): 82\n"
     ]
    }
   ],
   "source": [
    "# Filter 3: Filter recent earthquakes (2021-2022)\n",
    "df_recent = df_transformed.filter(col(\"Year\").isin(2021, 2022))\n",
    "print(f\"Recent earthquakes (2021-2022): {df_recent.count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b33de40a-a51b-49a7-9ba7-326888acb5f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+-------------+-------------+-------------+---------+--------------+-------------+\n|magnitude_category|earthquake_count|avg_magnitude|max_magnitude|min_magnitude|avg_depth|tsunami_events|magnitude_std|\n+------------------+----------------+-------------+-------------+-------------+---------+--------------+-------------+\n|          Moderate|             499|         6.67|          6.9|          6.5|    72.88|           194|         0.14|\n|             Major|             255|         7.32|          7.9|          7.0|    82.05|           100|        0.273|\n|             Great|              28|         8.28|          9.1|          8.0|    73.23|            10|        0.304|\n+------------------+----------------+-------------+-------------+-------------+---------+--------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# GroupBy Statistics by magnitude category\n",
    "magnitude_stats = df_transformed \\\n",
    "    .groupBy(\"magnitude_category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"earthquake_count\"),\n",
    "        spark_round(avg(\"magnitude\"), 2).alias(\"avg_magnitude\"),\n",
    "        spark_round(max(\"magnitude\"), 2).alias(\"max_magnitude\"),\n",
    "        spark_round(min(\"magnitude\"), 2).alias(\"min_magnitude\"),\n",
    "        spark_round(avg(\"depth\"), 2).alias(\"avg_depth\"),\n",
    "        spark_sum(col(\"tsunami\")).alias(\"tsunami_events\"),\n",
    "        spark_round(stddev(\"magnitude\"), 3).alias(\"magnitude_std\")\n",
    "    ) \\\n",
    "    .orderBy(\"earthquake_count\", ascending=False)\n",
    "\n",
    "magnitude_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81e17474-a734-4189-9580-91202e7ed81a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+-------------+---------------------+-----------------+------------------+\n|Year|total_earthquakes|tsunami_count|avg_tsunami_magnitude|avg_tsunami_depth|tsunami_percentage|\n+----+-----------------+-------------+---------------------+-----------------+------------------+\n|2022|               40|           32|                 6.86|           133.52|              80.0|\n|2021|               42|           33|                 7.09|            65.54|             78.57|\n|2020|               27|           15|                 7.09|             54.0|             55.56|\n|2019|               33|           26|                 6.88|            84.94|             78.79|\n|2018|               43|           33|                 6.99|            114.0|             76.74|\n|2017|               36|           27|                 6.85|             76.0|              75.0|\n|2016|               43|           31|                 7.02|            84.94|             72.09|\n|2015|               53|           33|                  6.9|            78.56|             62.26|\n|2014|               48|           40|                 6.87|            72.23|             83.33|\n|2013|               53|           34|                  6.9|            78.16|             64.15|\n|2012|               31|            0|                 NULL|             NULL|               0.0|\n|2011|               34|            0|                 NULL|             NULL|               0.0|\n|2010|               41|            0|                 NULL|             NULL|               0.0|\n|2009|               26|            0|                 NULL|             NULL|               0.0|\n|2008|               25|            0|                 NULL|             NULL|               0.0|\n|2007|               37|            0|                 NULL|             NULL|               0.0|\n|2006|               26|            0|                 NULL|             NULL|               0.0|\n|2005|               28|            0|                 NULL|             NULL|               0.0|\n|2004|               32|            0|                 NULL|             NULL|               0.0|\n|2003|               31|            0|                 NULL|             NULL|               0.0|\n+----+-----------------+-------------+---------------------+-----------------+------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    " #GroupBy THE Yearly tsunami statistics\n",
    "yearly_tsunami_stats = df_transformed \\\n",
    "    .groupBy(\"Year\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_earthquakes\"),\n",
    "        spark_sum(col(\"tsunami\")).alias(\"tsunami_count\"),\n",
    "        spark_round(avg(when(col(\"tsunami\") == 1, col(\"magnitude\"))), 2).alias(\"avg_tsunami_magnitude\"),\n",
    "        spark_round(avg(when(col(\"tsunami\") == 1, col(\"depth\"))), 2).alias(\"avg_tsunami_depth\")\n",
    "    ) \\\n",
    "    .withColumn(\"tsunami_percentage\", \n",
    "                spark_round(col(\"tsunami_count\") * 100.0 / col(\"total_earthquakes\"), 2)) \\\n",
    "    .orderBy(\"Year\", ascending=False)\n",
    "\n",
    "yearly_tsunami_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a28a353-652d-4ca3-83c6-485a662c81bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+-----------+-------------+--------------+-------------+\n|ocean_region|depth_category|event_count|avg_magnitude|tsunami_events|avg_intensity|\n+------------+--------------+-----------+-------------+--------------+-------------+\n|    Atlantic|       Shallow|        191|         6.96|            97|        69.31|\n|    Atlantic|  Intermediate|         33|         6.98|            17|        66.55|\n|    Atlantic|          Deep|         22|          6.9|            12|        51.64|\n|      Indian|       Shallow|         48|         6.85|            12|         66.1|\n|      Indian|  Intermediate|          9|         7.02|             2|         64.8|\n|     Pacific|       Shallow|        380|         6.94|           123|        57.94|\n|     Pacific|  Intermediate|         69|         6.91|            28|        55.26|\n|     Pacific|          Deep|         30|         6.99|            13|        54.99|\n+------------+--------------+-----------+-------------+--------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# GroupBy Regional analysis\n",
    "regional_analysis = df_transformed \\\n",
    "    .groupBy(\"ocean_region\", \"depth_category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"event_count\"),\n",
    "        spark_round(avg(\"magnitude\"), 2).alias(\"avg_magnitude\"),\n",
    "        spark_sum(\"tsunami\").alias(\"tsunami_events\"),\n",
    "        spark_round(avg(\"seismic_intensity\"), 2).alias(\"avg_intensity\")\n",
    "    ) \\\n",
    "    .orderBy([\"ocean_region\", \"event_count\"], ascending=[True, False])\n",
    "\n",
    "regional_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e252fa6-7473-485d-b94d-4f00f4f7fb23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_main = df_transformed.alias(\"main\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cfecaf5-4d67-4c5f-a5e0-6f17bd5da783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_after = df_transformed.alias(\"after\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdfcb256-ea28-4b23-89a1-6c0876e27a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential related earthquakes (mainshock-aftershock pairs):\n+--------------+--------+--------+----+----------+-----------------+-------------+---------------+--------------------+\n|main_magnitude|main_lat|main_lon|year|main_month|related_magnitude|related_month|related_tsunami|magnitude_difference|\n+--------------+--------+--------+----+----------+-----------------+-------------+---------------+--------------------+\n|           7.2| -38.355| -73.326|2011|         1|              6.7|            2|              0|                 0.5|\n|           7.2| -38.355| -73.326|2011|         1|              6.9|            2|              0|           0.2999997|\n|           7.7|  -0.414| 132.885|2009|         1|              7.4|            1|              0|           0.2999997|\n|           7.2|   -17.6| 167.856|2002|         1|              6.6|            1|              0|           0.5999999|\n|           7.5|   6.898| 126.579|2001|         1|              6.8|            1|              0|           0.6999998|\n|           7.1|  -8.783| 157.354|2010|         1|              6.8|            1|              0|           0.2999997|\n|           7.1|  -8.783| 157.354|2010|         1|              6.6|            1|              0|                 0.5|\n|           6.9|  36.501|   21.67|2008|         2|              6.5|            2|              0|           0.4000001|\n|           7.5|   1.065| 126.282|2007|         1|              6.7|            2|              0|           0.8000002|\n|           7.1|   5.293| 123.337|2005|         2|              6.5|            2|              0|           0.5999999|\n+--------------+--------+--------+----+----------+-----------------+-------------+---------------+--------------------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "related_earthquakes = df_main.join(\n",
    "    df_after,\n",
    "    (df_main[\"Year\"] == df_after[\"Year\"]) &\n",
    "    (spark_abs(df_main[\"Month\"] - df_after[\"Month\"]) <= 1) &\n",
    "    (spark_abs(df_main[\"latitude\"] - df_after[\"latitude\"]) < 5) &\n",
    "    (spark_abs(df_main[\"longitude\"] - df_after[\"longitude\"]) < 5) &\n",
    "    (df_main[\"magnitude\"] > df_after[\"magnitude\"]) &\n",
    "    (df_main[\"magnitude\"] >= 6.5),\n",
    "    \"inner\"\n",
    ") \\\n",
    ".select(\n",
    "    df_main[\"magnitude\"].alias(\"main_magnitude\"),\n",
    "    df_main[\"latitude\"].alias(\"main_lat\"),\n",
    "    df_main[\"longitude\"].alias(\"main_lon\"),\n",
    "    df_main[\"Year\"].alias(\"year\"),\n",
    "    df_main[\"Month\"].alias(\"main_month\"),\n",
    "    df_after[\"magnitude\"].alias(\"related_magnitude\"),\n",
    "    df_after[\"Month\"].alias(\"related_month\"),\n",
    "    df_after[\"tsunami\"].alias(\"related_tsunami\")\n",
    ") \\\n",
    ".withColumn(\"magnitude_difference\", \n",
    "            col(\"main_magnitude\") - col(\"related_magnitude\"))\n",
    "\n",
    "print(\"Potential related earthquakes (mainshock-aftershock pairs):\")\n",
    "related_earthquakes.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e8130c1-3853-4aab-9f8b-9ab3a3d89038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_transformed.createOrReplaceTempView(\"earthquakes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4be43cf4-390f-45eb-b77d-597764749716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql_query1 = \"\"\"\n",
    "    SELECT \n",
    "        magnitude,\n",
    "        depth,\n",
    "        latitude,\n",
    "        longitude,\n",
    "        Year,\n",
    "        Month,\n",
    "        sig as significance,\n",
    "        tsunami_risk_score,\n",
    "        magnitude_category,\n",
    "        ocean_region\n",
    "    FROM earthquakes\n",
    "    WHERE tsunami = 1\n",
    "    ORDER BY magnitude DESC, sig DESC\n",
    "    LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d1e948-c56d-4f3b-8e84-e2b7f6863fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Query 1: Top 10 tsunami-generating earthquakes by magnitude\n+---------+------+--------+---------+----+-----+------------+------------------+------------------+------------+\n|magnitude| depth|latitude|longitude|Year|Month|significance|tsunami_risk_score|magnitude_category|ocean_region|\n+---------+------+--------+---------+----+-----+------------+------------------+------------------+------------+\n|      8.3| 22.44|-31.5729| -71.6744|2015|    9|        1960| 85.24400196075439|             Great|    Atlantic|\n|      8.3| 598.1|  54.892|  153.221|2013|    5|        1115|142.80999946594238|             Great|     Pacific|\n|      8.2| 47.39| 15.0222| -93.8993|2017|    9|        2910|  86.7389980316162|             Great|    Atlantic|\n|      8.2|  25.0|-19.6097| -70.7691|2014|    4|        1332| 84.49999809265137|             Great|    Atlantic|\n|      8.2|  35.0| 55.3154| -157.829|2021|    7|        1252| 85.49999809265137|             Great|    Atlantic|\n|      8.2| 46.66| 55.4742| -157.917|2021|    7|        1237| 86.66599807739257|             Great|    Atlantic|\n|      8.2| 600.0|-18.1125| -178.153|2018|    8|        1050|141.99999809265137|             Great|    Atlantic|\n|      8.1| 28.93|-29.7466| -177.224|2021|    3|        1021| 83.89300384521485|             Great|    Atlantic|\n|      8.0|122.57| -5.8119| -75.2697|2019|    5|        1890| 92.25699996948242|             Great|    Atlantic|\n|      8.0|  24.0| -10.799|  165.114|2013|    2|         993|              82.4|             Great|     Pacific|\n+---------+------+--------+---------+----+-----+------------+------------------+------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"SQL Query 1: Top 10 tsunami-generating earthquakes by magnitude\")\n",
    "top_tsunamis = spark.sql(sql_query1)\n",
    "top_tsunamis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae79bc4e-dc5f-452e-9acf-7aa10912467d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL Query 2: Monthly statistics with window functions\n",
    "sql_query2 = \"\"\"\n",
    "    WITH monthly_stats AS (\n",
    "        SELECT \n",
    "            Year,\n",
    "            Month,\n",
    "            COUNT(*) as monthly_count,\n",
    "            AVG(magnitude) as avg_mag,\n",
    "            MAX(magnitude) as max_mag,\n",
    "            SUM(tsunami) as tsunami_count,\n",
    "            AVG(depth) as avg_depth\n",
    "        FROM earthquakes\n",
    "        GROUP BY Year, Month\n",
    "    )\n",
    "    SELECT \n",
    "        Year,\n",
    "        Month,\n",
    "        monthly_count,\n",
    "        ROUND(avg_mag, 2) as avg_magnitude,\n",
    "        max_mag as max_magnitude,\n",
    "        tsunami_count,\n",
    "        ROUND(avg_depth, 1) as avg_depth,\n",
    "        SUM(monthly_count) OVER (PARTITION BY Year ORDER BY Month) as cumulative_yearly_count,\n",
    "        RANK() OVER (PARTITION BY Year ORDER BY max_mag DESC) as magnitude_rank_in_year\n",
    "    FROM monthly_stats\n",
    "    ORDER BY Year DESC, Month DESC\n",
    "    LIMIT 20\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb3f7cb-e173-4ac6-8a8f-f3ef7215bc19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nSQL Query 2: Monthly statistics with window functions\n+----+-----+-------------+-------------+-------------+-------------+---------+-----------------------+----------------------+\n|Year|Month|monthly_count|avg_magnitude|max_magnitude|tsunami_count|avg_depth|cumulative_yearly_count|magnitude_rank_in_year|\n+----+-----+-------------+-------------+-------------+-------------+---------+-----------------------+----------------------+\n|2022|   11|            7|         6.94|          7.3|            6|    367.1|                     40|                     2|\n|2022|   10|            1|          6.7|          6.7|            1|     20.0|                     33|                     8|\n|2022|    9|            7|          7.0|          7.6|            6|     47.4|                     32|                     1|\n|2022|    8|            1|          6.6|          6.6|            1|     30.0|                     25|                     9|\n|2022|    7|            1|          7.0|          7.0|            1|     33.7|                     24|                     5|\n|2022|    6|            1|          6.5|          6.5|            0|    622.7|                     23|                    11|\n|2022|    5|            3|         6.97|          7.2|            3|    155.3|                     22|                     4|\n|2022|    4|            1|          6.6|          6.6|            1|     27.0|                     19|                     9|\n|2022|    3|            6|         6.87|          7.3|            5|     22.8|                     18|                     2|\n|2022|    2|            2|         6.65|          6.8|            1|    322.5|                     12|                     6|\n|2022|    1|           10|         6.61|          6.8|            7|     25.4|                     10|                     6|\n|2021|   12|            2|          7.3|          7.3|            1|     89.9|                     42|                     6|\n|2021|   11|            2|         7.05|          7.5|            2|     69.0|                     40|                     5|\n|2021|   10|            3|         7.03|          7.3|            3|    371.2|                     38|                     6|\n|2021|    9|            2|         6.75|          7.0|            2|     20.5|                     35|                     9|\n|2021|    8|           10|         7.12|          8.1|            9|     31.2|                     33|                     2|\n|2021|    7|            4|         7.45|          8.2|            4|     50.4|                     23|                     1|\n|2021|    6|            1|          6.5|          6.5|            1|     25.0|                     19|                    11|\n|2021|    5|            5|         6.82|          7.3|            2|     16.8|                     18|                     6|\n|2021|    4|            2|          6.5|          6.5|            0|    273.5|                     13|                    11|\n+----+-----+-------------+-------------+-------------+-------------+---------+-----------------------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSQL Query 2: Monthly statistics with window functions\")\n",
    "monthly_analysis = spark.sql(sql_query2)\n",
    "monthly_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c4f085a-304b-48b1-bb6e-4a3d30e8cabc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL Query 3: Complex analysis with CTEs\n",
    "sql_query3 = \"\"\"\n",
    "    WITH depth_categories AS (\n",
    "        SELECT \n",
    "            CASE \n",
    "                WHEN depth < 70 THEN 'Shallow'\n",
    "                WHEN depth < 300 THEN 'Intermediate'\n",
    "                ELSE 'Deep'\n",
    "            END as depth_cat,\n",
    "            magnitude,\n",
    "            tsunami,\n",
    "            sig\n",
    "        FROM earthquakes\n",
    "    ),\n",
    "    category_stats AS (\n",
    "        SELECT \n",
    "            depth_cat,\n",
    "            COUNT(*) as count,\n",
    "            AVG(magnitude) as avg_mag,\n",
    "            STDDEV(magnitude) as std_mag,\n",
    "            SUM(tsunami) as tsunami_total,\n",
    "            AVG(sig) as avg_significance\n",
    "        FROM depth_categories\n",
    "        GROUP BY depth_cat\n",
    "    )\n",
    "    SELECT \n",
    "        depth_cat as depth_category,\n",
    "        count as earthquake_count,\n",
    "        ROUND(avg_mag, 3) as average_magnitude,\n",
    "        ROUND(std_mag, 3) as magnitude_std_dev,\n",
    "        tsunami_total,\n",
    "        ROUND(100.0 * tsunami_total / count, 2) as tsunami_percentage,\n",
    "        ROUND(avg_significance, 1) as avg_significance_score\n",
    "    FROM category_stats\n",
    "    ORDER BY earthquake_count DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d111917-6ff8-463b-ad71-70d0446b3ab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nSQL Query 3: Statistical analysis by depth category\n+--------------+----------------+-----------------+-----------------+-------------+------------------+----------------------+\n|depth_category|earthquake_count|average_magnitude|magnitude_std_dev|tsunami_total|tsunami_percentage|avg_significance_score|\n+--------------+----------------+-----------------+-----------------+-------------+------------------+----------------------+\n|       Shallow|             619|             6.94|            0.455|          232|             37.48|                 883.2|\n|  Intermediate|             111|             6.94|            0.386|           47|             42.34|                 847.2|\n|          Deep|              52|            6.956|            0.452|           25|             48.08|                 763.1|\n+--------------+----------------+-----------------+-----------------+-------------+------------------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSQL Query 3: Statistical analysis by depth category\")\n",
    "depth_analysis = spark.sql(sql_query3)\n",
    "depth_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd44aca-90e4-432c-accd-2d59eabd819c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First action (count): 782 rows in 0.389 seconds\nSecond action (avg): 75.88 depth in 0.321 seconds\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate impact of caching\n",
    "start = time.time()\n",
    "count1 = df_transformed.filter(col(\"magnitude\") > 6.0).count()\n",
    "time1 = time.time() - start\n",
    "print(f\"First action (count): {count1} rows in {time1:.3f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "avg1 = df_transformed.filter(col(\"magnitude\") > 6.0).agg(avg(\"depth\")).collect()[0][0]\n",
    "time2 = time.time() - start\n",
    "print(f\"Second action (avg): {avg1:.2f} depth in {time2:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4dc579-cdae-470d-9f48-6e65a1116c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inefficient approach - filter after operations:\nTime: 0.460 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inefficient query (filter after join)\n",
    "print(\"Inefficient approach - filter after operations:\")\n",
    "start = time.time()\n",
    "inefficient = df_transformed \\\n",
    "    .select(\"*\") \\\n",
    "    .withColumn(\"complex_calc\", col(\"magnitude\") * col(\"sig\") * col(\"depth\")) \\\n",
    "    .filter(col(\"magnitude\") > 7.0) \\\n",
    "    .count()\n",
    "inefficient_time = time.time() - start\n",
    "print(f\"Time: {inefficient_time:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a7112c-6a99-407c-9023-37f7414350ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation 1 (filter): 0.0003 seconds\n"
     ]
    }
   ],
   "source": [
    "# transformations \n",
    "trans_start = time.time()\n",
    "lazy_df1 = df_raw.filter(col(\"magnitude\") > 5.0)\n",
    "print(f\"Transformation 1 (filter): {time.time() - trans_start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "915e9d39-282a-4d9c-9326-274a0389da1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation 2 (select): 0.0003 seconds\nTransformation 3 (withColumn): 0.0002 seconds\nTransformation 4 (groupBy + agg): 0.0002 seconds\n"
     ]
    }
   ],
   "source": [
    "trans_start = time.time()\n",
    "lazy_df2 = lazy_df1.select(\"magnitude\", \"depth\", \"tsunami\")\n",
    "print(f\"Transformation 2 (select): {time.time() - trans_start:.4f} seconds\")\n",
    "\n",
    "trans_start = time.time()\n",
    "lazy_df3 = lazy_df2.withColumn(\"magnitude_squared\", col(\"magnitude\") ** 2)\n",
    "print(f\"Transformation 3 (withColumn): {time.time() - trans_start:.4f} seconds\")\n",
    "\n",
    "trans_start = time.time()\n",
    "lazy_df4 = lazy_df3.groupBy(\"tsunami\").agg(avg(\"magnitude_squared\"))\n",
    "print(f\"Transformation 4 (groupBy + agg): {time.time() - trans_start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a50b20-9bc5-4ac5-9e79-18b70d4e7542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML Dataset size: 782 records\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning\n",
    "ml_data = df_transformed.select(\n",
    "    \"magnitude\", \"depth\", \"cdi\", \"mmi\", \"sig\", \"nst\", \n",
    "    \"dmin\", \"gap\", \"latitude\", \"longitude\", \"tsunami\"\n",
    ").na.drop()  # Remove any null values\n",
    "\n",
    "print(f\"ML Dataset size: {ml_data.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec92bfd-0302-4014-a473-40c5ef76bea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation 1 (filter): 0.0003 seconds\nTransformation 2 (select): 0.0005 seconds\nTransformation 3 (withColumn): 0.0002 seconds\nTransformation 4 (groupBy + agg): 0.0002 seconds\n\nNo actual computation performed yet - just building execution plan!\n"
     ]
    }
   ],
   "source": [
    "# These are transformations - (Lazy - No Execution)\n",
    "trans_start = time.time()\n",
    "lazy_df1 = df_raw.filter(col(\"magnitude\") > 5.0)\n",
    "print(f\"Transformation 1 (filter): {time.time() - trans_start:.4f} seconds\")\n",
    "\n",
    "trans_start = time.time()\n",
    "lazy_df2 = lazy_df1.select(\"magnitude\", \"depth\", \"tsunami\")\n",
    "print(f\"Transformation 2 (select): {time.time() - trans_start:.4f} seconds\")\n",
    "\n",
    "trans_start = time.time()\n",
    "lazy_df3 = lazy_df2.withColumn(\"magnitude_squared\", col(\"magnitude\") ** 2)\n",
    "print(f\"Transformation 3 (withColumn): {time.time() - trans_start:.4f} seconds\")\n",
    "\n",
    "trans_start = time.time()\n",
    "lazy_df4 = lazy_df3.groupBy(\"tsunami\").agg(avg(\"magnitude_squared\"))\n",
    "print(f\"Transformation 4 (groupBy + agg): {time.time() - trans_start:.4f} seconds\")\n",
    "\n",
    "print(\"\\nNo actual computation performed yet - just building execution plan!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df2394a-ac7e-4cf7-8697-dd82ab41d3da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 633 records\nTest set: 149 records\n\nClassification Model: Tsunami Prediction\nTraining Random Forest model...\nTraining completed in 3.80 seconds\nModel AUC-ROC: 0.7246\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"magnitude\", \"depth\", \"sig\", \"latitude\", \"longitude\", \"gap\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "train_data, test_data = ml_data.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training set: {train_data.count()} records\")\n",
    "print(f\"Test set: {test_data.count()} records\")\n",
    "\n",
    "print(\"\\nClassification Model: Tsunami Prediction\")\n",
    "\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"tsunami\",\n",
    "    numTrees=100,\n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "#  pipeline\n",
    "classification_pipeline = Pipeline(stages=[assembler, scaler, rf_classifier])\n",
    "\n",
    "# Train model\n",
    "print(\"Training Random Forest model...\")\n",
    "start = time.time()\n",
    "rf_model = classification_pipeline.fit(train_data)\n",
    "print(f\"Training completed in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"tsunami\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Model AUC-ROC: {auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment_PySpark Data Processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}